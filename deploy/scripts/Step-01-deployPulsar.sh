#!/bin/bash

# The following scripting is based upon a Pulsar tutorial for running Pulsar in Kubernetes on
# minikube. The url for the tuturial is:
# https://pulsar.apache.org/docs/4.0.x/getting-started-helm/

# Step 1 - Deploys and configures the following:
#            a) Deploys a fresh minikube with minikube addons (dashboard, metallb);
#            c) Configures Metallb loadbalancer
#            d) Deploys Kubernetes Gateway API CRDs (cert-manager deploy uses)
#            e) Deploys istio in Ambient mode.
#            f) Deploys Cert-Manager
#            g) Deploys Pulsar and all required components into the Cluster
#            h) Sets pulsar namespace to istio ambient mode which initiates mTLS between pods
#            f) Tests access from the Pulsar CLI client
#            g) Allows running of the simple java test program (eclipse, maven) found in the
#               pulsar-client directory within this project.

# This learning prototypes were developed and tested using the following:
#   a) Ubuntu                 - 20.04.6 LTS
#   b) Minikube               - 1.34.0
#   c) Kubernetes             - 1.31.0
#   d) Docker                 - 27.2.0
#   e) Metallb                - 0.9.6
#   f) Kubernetes Gateway API - 1.2.0
#   g) Istio (Ambient Mode)   - 1.23.2
#   h) Cert-Manager           - 1.15.5
#   i) Machine config - Processor - Intel® Core™ i7-7700K CPU @ 4.20GHz × 8 
#                       Memory    - 64 GB
#  
# Pulsar deployment
# The following deployment was initially generated by running a dry-run from the Pulsar
# minikube Helm chart as follows:
# helm install --dry-run --values ${PROTODIR}/helm/values-02.yaml --namespace pulsar pulsar-mini apache/pulsar > kube-pulsar.txt
# From this output the kube deployment components were obtained. This script does a minimal
# installation of only the main required Pulsar components.
#   Component   # Deployed
#    Zookeeper       3
#    Bookie          4
#    Toolset         1
#    Broker          3
#    Proxy           3
# Only the basic elements are deployed - so no PodMonitor, Prometheus, Graphana, PodDisrutpion, etc.
#
#################################################################################################
##### Note - Only if it has not already been installed to your dev machine do you need to do this step.
# Install pulsarctl from streamnative - Installs to $HOME/.pulsarctl
sudo bash -c "$(curl -fsSL https://raw.githubusercontent.com/streamnative/pulsarctl/master/install.sh)"
#################################################################################################
#         
# Open terminal 1
# Delete prior minikube ( if used and configured prior)
minikube delete -p servers

# Start minikube - configure the settings to your requirements and hardware
# Note - normally I use kvm2 as the vm-driver. However istio cni in ambient mode does not
# currently work with kvm2 due to cni incompatibility. The work around is to use the 
# docker vm-driver.
minikube start -p servers --cpus 3 --memory 12288 --vm-driver docker --cni kindnet --disk-size 100g

# Not really necessary as the minikube start configures kubectl to the servers instance.
minikube profile servers

# Addons
minikube addons enable dashboard

# Deploy the addon loadbalancer metallb
minikube addons enable metallb

# Configure loadbalancer ip address range within the same range as the minikube ip
# The configuration is a start ip ( ie. 192.168.49.20 ) and an end ip that makes a 
# range of 10 ip addresses. The range should not overlap the minikube ip
minikube ip
minikube addons configure metallb
  -- Enter Load Balancer Start IP: 
  -- Enter Load Balancer End IP:

# Start dashboard
minikube dashboard

############## Open up a new (2nd) terminal ###################################
# Install the Kubernetes Gateway API CRDs (experimental also includes standard)
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/experimental-install.yaml

# Install istio in ambient mode
istioctl install --set values.pilot.env.PILOT_ENABLE_ALPHA_GATEWAY_API=true --set profile=ambient --skip-confirmation

#### Install cert-manager with the following steps ####
# Create cert-manger namespace
kubectl create namespace cert-manager

# Deploy cert-manager gateway CRDs
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.5/cert-manager.crds.yaml

# Deploy cert-manager with gateway api enabled including the experimental gateway apis
helm install cert-manager --version 1.15 jetstack/cert-manager --namespace cert-manager \
  --set config.apiVersion="controller.config.cert-manager.io/v1alpha1" \
  --set config.kind="ControllerConfiguration" \
  --set config.enableGatewayAPI=true \
  --set "extraArgs={--feature-gates=ExperimentalGatewayAPISupport=true}"

# Project Directory - Change to your source directory
PROTODIR=/media/tim/ExtraDrive1/Projects/009-SecureKeyAndCertRotation/deploy

# Create pulsar namespace
kubectl create namespace pulsar

# Generate token keys and a secret for the pulsar cluster and super-users
$PROTODIR/pulsar-helm-chart/scripts/pulsar/prepare_helm_release.sh -n pulsar -k pulsar

# Deploy cert-manager tls self signed issuer and certificates
kubectl apply -f ${PROTODIR}/kube-pulsar/proxy-cert-issuer.yaml -n pulsar
kubectl apply -f ${PROTODIR}/kube-pulsar/proxy-certificates.yaml -n pulsar

kubectl apply -f ${PROTODIR}/kube-pulsar/pulsar-configmaps.yaml

# No pulsar deployment dependencies
kubectl apply -f ${PROTODIR}/kube-pulsar/zookeeper-service.yaml
kubectl apply -f ${PROTODIR}/kube-pulsar/zookeeper-statefulset.yaml
kubectl -n pulsar wait --timeout=10m --for=condition=Ready pod/pulsar-zookeeper-0

# Has no Pulsar deployment dependencies
kubectl apply -f ${PROTODIR}/kube-pulsar/toolset-service.yaml
kubectl apply -f ${PROTODIR}/kube-pulsar/toolset-statefulset.yaml

# Initialize bookie cluster within zookeeper Job
kubectl apply -f ${PROTODIR}/kube-pulsar/bookie-service.yaml
kubectl apply -f ${PROTODIR}/kube-pulsar/bookie-init-job.yaml

# initialize pulsar metadata after bookie-init-job completed and succeeded
kubectl exec -it -n pulsar pulsar-toolset-0 -- /bin/sh
# From within toolset shell
export PULSAR_MEM="-Xmx128M";
bin/pulsar initialize-cluster-metadata \
              --cluster pulsar \
              --zookeeper pulsar-zookeeper:2181 \
              --configuration-store pulsar-zookeeper:2181 \
              --web-service-url http://pulsar-broker.pulsar.svc.cluster.local:8080/ \
              --web-service-url-tls https://pulsar-broker.pulsar.svc.cluster.local:8443/ \
              --broker-service-url pulsar://pulsar-broker.pulsar.svc.cluster.local:6650/ \
              --broker-service-url-tls pulsar+ssl://pulsar-broker.pulsar.svc.cluster.local:6651/;

exit


# Requires zookeeper to be up and ready and the pulsar initialize cluster metadata cmd
# to be run - Note bookie init job and bookie service already started above.
kubectl apply -f ${PROTODIR}/kube-pulsar/bookie-statefulset.yaml
kubectl -n pulsar wait --timeout=1m --for=condition=Ready pod/pulsar-bookie-2

# Requires Zookeeper and Bookie ready as a dependency
kubectl apply -f ${PROTODIR}/kube-pulsar/broker-service.yaml
kubectl apply -f ${PROTODIR}/kube-pulsar/broker-statefulset.yaml
kubectl -n pulsar wait --timeout=2m --for=condition=Ready pod/pulsar-broker-2

# Has Zookeeper and Broker ready dependency
kubectl apply -f ${PROTODIR}/kube-pulsar/proxy-service.yaml
kubectl apply -f ${PROTODIR}/kube-pulsar/proxy-statefulset.yaml
kubectl -n pulsar wait --timeout=1m --for=condition=Ready pod/pulsar-proxy-2

# To verify everything is up and running
kubectl get pods -n pulsar
kubectl get services -n pulsar

# Create tenants, namespaces and topics from within the toolset container
kubectl exec -it -n pulsar pulsar-toolset-0 -- /bin/bash

# Create a tenant
bin/pulsar-admin tenants create metadata
bin/pulsar-admin tenants list

# Create the pulsar namespaces
bin/pulsar-admin namespaces create metadata/client
bin/pulsar-admin namespaces create metadata/kyber
bin/pulsar-admin namespaces create metadata/pulsar
bin/pulsar-admin namespaces create metadata/watcher
bin/pulsar-admin namespaces list metadata

# Create a test topic with 3 partitions
bin/pulsar-admin topics create-partitioned-topic metadata/client/request            -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/pulsar/tls-observer       -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/pulsar/tls-publish        -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/kyber/exchange-request    -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/watcher/tls-update        -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/watcher/exchange-request  -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/watcher/exchange-response -p 3
bin/pulsar-admin topics create-partitioned-topic metadata/watcher/cert-update       -p 3

bin/pulsar-admin topics list-partitioned-topics metadata/client
bin/pulsar-admin topics list-partitioned-topics metadata/pulsar
bin/pulsar-admin topics list-partitioned-topics metadata/kyber
bin/pulsar-admin topics list-partitioned-topics metadata/watcher

exit

# Set pulsar namespace to istio ambient mode (ie. no sidecar) also initiates mTls
kubectl label namespace pulsar istio.io/dataplane-mode=ambient

# Start a new Terminal for the port-forward using the servers profile
# Ensure Servers cluster 
minikube profile servers

#kubectl port-forward -n pulsar service/pulsar-proxy 6651:6651 --address=0.0.0.0


